{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a89fa088-455d-4234-af59-c0054c724e2f",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0adc4065b1b8db86b577781b2de5fef",
     "grade": false,
     "grade_id": "cell-12059b68c07e81bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Spark MLlib\n",
    "## Lab assignment: Example with the MLlib\n",
    "\n",
    "The aim of this notebook is to play with the MLlib of Apache Spark to create a Machine Learning pipeline that preprocess a dataset, train a model and make predictions. In particular, we are going to deal with a regression problem. The content of this lab has been inspired by the sample provided in the [DataBricks documentation](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2854662143668609/2084788691983918/6837869239396014/latest.html).\n",
    "\n",
    "\n",
    "## Appliances Energy Prediction\n",
    "\n",
    "**Data**: This dataset contains Energy consumption from appliances at 10 min resolution for about 4.5 months. The house temperature and humidity conditions were monitored with a wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes. Weather from the nearest airport weather station (Chievres Airport, Belgium) was also logged. This dataset is from [Candanedo et al](http://dx.doi.org/10.1016/j.enbuild.2017.01.083) and is hosted by the UCI Machine Learning Repository. [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction).\n",
    "\n",
    "\n",
    "**Goal**: We want to learn to predict appliances' energy consumption based on weather information. It would also be nice to know which input features are the most relevant to make predictions.\n",
    "\n",
    "**Approach**: We will use Spark ML Pipelines, which help users piece together parts of a workflow such as feature processing and model training. We will also demonstrate model selection (a.k.a. hyperparameter tuning) using Cross Validation in order to fine-tune and improve our ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15f422e7-fc3c-417e-8c83-cc7fd25993e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Submission and marking criteria\n",
    "\n",
    "You should complete this notebook and add your solutions to it. When you are done, rename your completed notebook as `ex04.ipynb`. \n",
    "\n",
    "Important notes:\n",
    "- The **group leader** must submit the `ex04.ipynb` file on Moodle.\n",
    "- **Each member of the group** must complete the peer review survey and their contribution statement using this [link](https://forms.office.com/Pages/ResponsePage.aspx?id=7qe9Z4D970GskTWEGCkKHjZupmfSK6JKqlvGZrucaoBURFJJWllYWVhQS09PMFNBVzlCT05JUjM4VCQlQCN0PWcu). **You can only submit this survey ONCE**.\n",
    "- This lab is marked out of 100 marks, and each section is allocated a number of marks that are indicated below.\n",
    "- The marking will be focused on the efficiency of the solution and its functionality. Minor mistakes will deduct marks from each exercise.\n",
    "\n",
    "- **Submission deadline: 25th March 2022 at 3pm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4d24469-3c16-4bf9-baf7-7793daace361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Helper function to test the correctness of the solutions\n",
    "def test(var, val, msg=\"\"):\n",
    "    print(\"1 test passed.\") if var == val else print(\"1 test failed. \" + msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24d6f32f-29f2-4514-b4f8-dbc4bcbfa3fc",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "632d5f2de7960dfc5d178a675e8e7499",
     "grade": false,
     "grade_id": "cell-8937897c4473abfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Load and understand the data  [10 marks]\n",
    "\n",
    "We begin by loading the data, which is in Comma-Separated Value (CSV) format. For that, you should use `spark.read` to read the file. Then, you should also cache the data so that we only read it from disk once. You will need to upload the data to your group's storage accout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e87de52d-387c-4ad4-a4a4-be3700c5fcac",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6152d43a83ada31996ee66e1f65cc4a8",
     "grade": false,
     "grade_id": "cell-3aadc724e217ea88",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").csv(\"/mnt/data/energydata_complete.csv\")\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "19320376-04af-4f56-93df-f49a9d917a86",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7d9cebbd79c647f5e4c382bedb3ea68",
     "grade": true,
     "grade_id": "cell-28b65c981c65a16d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(df.count(), 19735, 'Incorrect number of rows')\n",
    "test(df.is_cached, True, 'df not cached')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c258f89a-3196-4c89-81b9-d1d5f90dc11e",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bf8ff1c31febe73356ef34d9909c796",
     "grade": false,
     "grade_id": "cell-c8a8f77c243d18c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Data description\n",
    "\n",
    "From the UCI ML Repository description, we know that the columns have the following meanings.\n",
    "\n",
    "**Attribute information**:\n",
    "```\n",
    "date time year-month-day hour:minute:second\n",
    "Appliances, energy use in Wh\n",
    "lights, energy use of light fixtures in the house in Wh\n",
    "T1, Temperature in kitchen area, in Celsius\n",
    "RH_1, Humidity in kitchen area, in %\n",
    "T2, Temperature in living room area, in Celsius\n",
    "RH_2, Humidity in living room area, in %\n",
    "T3, Temperature in laundry room area\n",
    "RH_3, Humidity in laundry room area, in %\n",
    "T4, Temperature in office room, in Celsius\n",
    "RH_4, Humidity in office room, in %\n",
    "T5, Temperature in bathroom, in Celsius\n",
    "RH_5, Humidity in bathroom, in %\n",
    "T6, Temperature outside the building (north side), in Celsius\n",
    "RH_6, Humidity outside the building (north side), in %\n",
    "T7, Temperature in ironing room , in Celsius\n",
    "RH_7, Humidity in ironing room, in %\n",
    "T8, Temperature in teenager room 2, in Celsius\n",
    "RH_8, Humidity in teenager room 2, in %\n",
    "T9, Temperature in parents room, in Celsius\n",
    "RH_9, Humidity in parents room, in %\n",
    "To, Temperature outside (from Chievres weather station), in Celsius\n",
    "Pressure (from Chievres weather station), in mm Hg\n",
    "RH_out, Humidity outside (from Chievres weather station), in %\n",
    "Wind speed (from Chievres weather station), in m/s\n",
    "Visibility (from Chievres weather station), in km\n",
    "Tdewpoint (from Chievres weather station), Â°C\n",
    "rv1, Random variable 1, nondimensional\n",
    "rv2, Random variable 2, nondimensional\n",
    "```\n",
    "\n",
    "**The target variable is the energy use of the Appliances.**\n",
    "\n",
    "For now, we will leave the two variables `rv1` and `rv2` in our dataset, to see if they are affecting much our methods, then we can try to remove them and see if we improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7a781e82-47e4-47e9-bb4a-c3ae656612b5",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88d022c810ed35b8e96cbf869d6e7f7b",
     "grade": false,
     "grade_id": "cell-5c3b6f3c781e12af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the `display` (databricks only) or `show` commands to visualise the data. \n",
    "- Remember if you use `display` you can also easily make graphs\n",
    "- If you are using `show` be careful not to show the entire data frame :-), only 5 rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b4af1a68-0405-4086-a8e0-c5f39083469b",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "878e1d8e3b57f231ba0b3d5f11fcf209",
     "grade": false,
     "grade_id": "cell-0bc3c84df797cc37",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb1cb356-f382-4591-a172-007506b65160",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05cd3b9f24849db9e42ef79841c0e720",
     "grade": false,
     "grade_id": "cell-6a7113523db1a125",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Data preprocessing [10 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15986f8e-df70-4f36-b31c-bafc5c70158e",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "970010c7e6d30c24b9ca1ad857187d49",
     "grade": false,
     "grade_id": "cell-74870c89983edfb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This dataset is nicely prepared for Machine Learning and required very little preprocessing. However, rather than keeping the date as a timestamp, we would like to have some additional columns, including 'day of the year', 'hour', and 'month of the year'. Please use the naming `dayofyear`, `hour` and `month`, respectively.\n",
    "\n",
    "**Hint**: Of course the SparkSQL library has a function to transform strings with datetime!\n",
    "\n",
    "Would you like to have any other information from the datetime? Feel free to add other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ec18a11-2eb7-4c5f-a659-6adcc376dfda",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07e65cfc87797b262029012fc92ea89a",
     "grade": false,
     "grade_id": "cell-1f648a6d01eb2952",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('dayofyear', F.lit(F.dayofyear('date'))) \\\n",
    "                    .withColumn('hour', F.lit(F.hour('date'))) \\\n",
    "                    .withColumn('month', F.lit(F.month('date')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a727ab8a-17b1-40e1-af6d-7573fd0ce8e9",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a237d70ebe45116a4ab9989acab8793a",
     "grade": false,
     "grade_id": "cell-2561535e29742ef2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When you dataframe `df` has the additional columns, please remove the column `date`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff1cefcb-3e5a-4c26-a4e2-2735991ef4c6",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd69e01e8acc0c2713da407a568d68ec",
     "grade": false,
     "grade_id": "cell-96bed972cec4c7a1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70844a96-f4bb-4283-8f98-ef3f473c8f22",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "953bd28b595e05ed7f44ab32a6fea599",
     "grade": true,
     "grade_id": "cell-78b3ae64d5564833",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(\"date\" in df.columns, False, \"Column date has not been remove!\")\n",
    "test(\"hour\" in df.columns, True, \"The hour hasn't been added\")\n",
    "test(\"dayofyear\" in df.columns, True, \"The dayofyear hasn't been added\")\n",
    "test(\"month\" in df.columns, True, \"The month hasn't been added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "506ce213-8d24-4ecd-b2ea-0503947f3ff1",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49da810aa91d5bc53aaa1808269c1c68",
     "grade": false,
     "grade_id": "cell-2e4f9de42e7d939c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check the schema of your dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e891837c-eccb-4780-985a-b8b3511dbfa9",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c81ba660837e2c443af0b45e2927987d",
     "grade": false,
     "grade_id": "cell-f5ee1e564ac14dff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5fb969dc-a4bb-4d3b-82cd-9fd020af31a5",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fde4e3d21705e439d696d67b156e5db4",
     "grade": false,
     "grade_id": "cell-6bb2857862d64352",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Dammit, all the input features have been inferred as strings rather than numeric values. This is because we read the data from a CSV file with the data in between quotes.\n",
    "\n",
    "Your task now is to transform that into numerical values. All of the features are actually numeric, so you could cast all of them. You are recommended to use functions like `cast` and `col` to do this. You could try to leave out the datetime columns we created, but it's fine if you transform them to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "97aaf491-a7e3-49c8-9b71-1a215e0c997e",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc404f3266ba831cad5a3757dd1a204e",
     "grade": false,
     "grade_id": "cell-f72f135db795ec70",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for c in df.columns[:-3]:\n",
    "    # add condition for the cols to be type cast\n",
    "    df = df.withColumn(c, df[c].cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1e0d0c8-a2b7-469f-a724-93c2248e7aad",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05cde717adfaa2f94bf219ef440e1ec9",
     "grade": false,
     "grade_id": "cell-6291c14f0a17fcf0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "abd8d67f-c52c-49cf-88f9-b2aebfe88364",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9aa8da36c2f5aad4873cd224f39fc192",
     "grade": false,
     "grade_id": "cell-c937a928bbbec3d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Split data into training and test sets [10 marks]\n",
    "\n",
    "Our final data preparation step is to split our dataset into training and test sets. We will train and tune our model on the training set, and then see how well we do in the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "12bdb207-cd23-4250-ac10-fed0acf5ec45",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f65b3344ebf606806a358c7646592cd",
     "grade": false,
     "grade_id": "cell-557ed884643dbca0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Your task is to split the dataframe `df` into 70% for training and 30% for test. Please use the same random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bd8a257c-66dc-4b5c-ac86-e83827151189",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb421493c6c4f2ab70a5af3b768541a6",
     "grade": false,
     "grade_id": "cell-56849a52128319c8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c08b511f-19b7-4ce8-8eb3-58d37c7d032f",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de538983c5838ffdcae437945b597825",
     "grade": false,
     "grade_id": "cell-1fb4a0da2280473a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Even though we have fixed the random seed, you will not get the exact same split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad9bdfd3-afc7-468d-8292-c55cea3a2cb0",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6221ab1e4248679667e9c967a15b7d1",
     "grade": false,
     "grade_id": "cell-d4c519e050e792fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that this is the simplest way of validating your results. You may want to carry out a [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) and split the dataset into *k* folds, and build and test *k* models. We will do later cross validation but for parameter tuning! not to validate our approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8e18e497-854c-40a2-a0e0-97bce1959f28",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a40fd1e90f18d39214e084d3a966dee6",
     "grade": false,
     "grade_id": "cell-254b4625d65edbab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data visualisation [10 marks]\n",
    "\n",
    "Before applying any machine learning algorithm, it is a good practice to try to visualise your data. For example, we could see how much energy is spent in appliances depending on the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76c7a27a-6541-430c-8056-d8e6a9c6732c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ec7cb18-6290-4b86-922c-e54c2b4e1f3c",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8415b6e2ff9d674308ace1f95366681e",
     "grade": false,
     "grade_id": "cell-5521c415bf5963be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a variable `hist_elect` that contains the histogram of total energy consumed by the Appliances, grouped by month\n",
    "hist_elect = df.groupBy(\"month\").sum(\"Appliances\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf0f72ef-bbed-4154-b99f-ce00ee192ec0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(x_values, y_values) = zip(*hist_elect)\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Histogram of repetitions (up to 10)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sum of Appliance Energy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0411e974-092d-47eb-a97f-0da12be07362",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83748e0da71c609d736c2160a1b04e30",
     "grade": false,
     "grade_id": "cell-75f1197b0325c06d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we don't seem to have all the data from the 1st of January, well, we have less consumption in that particular months, so probably the month is not a good feature, don't you think? we are going to remove it from the dataframe and the training and test data frames too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eba2c6ea-f2f3-4f9d-9d97-6133e2833f87",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cc65609cd209c7db20af9ef09de38ac",
     "grade": false,
     "grade_id": "cell-fad68e0bbfd1e59f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# update the variables `df`, `train` and `test`, removing the column 'month'\n",
    "df = df.filter((df.month != '1'))\n",
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8ec1ebf6-8b9b-4697-a9e0-d98ce01d0734",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af460873762e3c8617ffb7f7580eac0b",
     "grade": false,
     "grade_id": "cell-d8aab95c80f20932",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You could do other plots to understand better the data and practice with Spark :-). This is a good opportunity to practice with the DataFrame API, although if you are running this on Databricks, some plots can be generated very easily by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da016c33-7aa0-4540-9f14-e005f6363be0",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a49870c2cc216fe0d7e91b74a868e21",
     "grade": true,
     "grade_id": "cell-13f6aa1e22b192f1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hist_elect2 = df.groupBy(\"month\").sum(\"Appliances\").collect()\n",
    "(x_values, y_values) = zip(*hist_elect2)\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Histogram of repetitions (up to 10)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sum of Appliance Energy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab71874d-ce0d-4d79-8b00-dc3a667abaa6",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d4048ac5928490fe33a851497606173",
     "grade": false,
     "grade_id": "cell-e49995e14a31fa47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Create a Pipeline with Spark ML [20 marks]\n",
    "\n",
    "As you know, we can't feed the data frame directly to a machine learning algorithm, as we need to put all the input features as an Array, and indicate which one is the output feature (in our case, the 'Appliances' column!).\n",
    "\n",
    "We will put together a simple Pipeline with the following stages:\n",
    "\n",
    "- VectorAssembler: To combine all the input columns into a single vector column (i.e. all the columns but the 'Appliances' one.\n",
    "- Learning algorithm: I feel like using Gradient-Boosted Trees [GBTs](https://en.wikipedia.org/wiki/Gradient_boosting) for this example, but feel free to use anything else.\n",
    "- CrossValidator: I will use cross validation to tune the parameters of the GBT model. Yes, this can be added as part of a pipeline!  This is going to change the way we access the best model later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24be6d54-955c-43be-a340-18c6a1336379",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "807c3b691e51f5dca0e276c18f79b137",
     "grade": false,
     "grade_id": "cell-3f24c59f54a6cf35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Step 1: Create the `VectorAssembler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57a03262-5aa5-4828-9766-530c4109a3cb",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a64a21121df4fea431a96eb1a193767",
     "grade": true,
     "grade_id": "cell-706de5b087873dcc",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = train.columns\n",
    "feature_cols.remove(\"Appliances\")\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembler.transform(train).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7937fbc-18f1-4bde-90fb-e3dd6a4c2145",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e24f36f23ecaf217b3432950daeb9c66",
     "grade": false,
     "grade_id": "cell-68a3d374f4a70c38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Step 2: Create an instance of `GBTRegressor` in which you don't indicate any parameters but the class label (i.e. `labelCol`) to 'Appliances'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "656c9f0e-68bc-4edc-bd0e-e63857a09790",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7092e1add42c762bdf9ee6e5b4805438",
     "grade": true,
     "grade_id": "cell-8db3898a09503ca1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"Appliances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ddeb1f79-3073-4077-a90b-9c79cf7898df",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae0da55026f2e2364b2c86baa0321b22",
     "grade": false,
     "grade_id": "cell-102cc339bfa52fe2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Step 3: Create a `CrossValidator` for `gbt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f82bc0d-de42-47cb-8e31-bd35de961fc6",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7db6e70f1b1d7db78c5d932f1a793b4c",
     "grade": false,
     "grade_id": "cell-a46ae0c64349795f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can explore the parameters you like for GBT. Full documentation [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GBTRegressor.html). I would suggest to create a 'grid' for at least the depth of the tree and the number of iterations. Don't investigate more than 4-8 combinations! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76fdb3ce-9789-4bdb-bf92-872606483e5c",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68b41ab2e6c2018ea7179476cecf99a9",
     "grade": false,
     "grade_id": "cell-154bc279b7bc636a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Import the right libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df8e0549-d2b6-47dd-a055-ec9018b66903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f45dae40-8020-4e57-84ca-2deac9230bec",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc15a04567a0efe4bfbc8036616d723b",
     "grade": true,
     "grade_id": "cell-bc9e2db69dde356f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# define a variable `paramGrid` with some parameters, e.g. for maxDepth, range [5,8], and for maxIter [10,20]\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [2]) \\\n",
    "    .addGrid(gbt.maxIter, [5,8,10,20]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57187607-b8a1-484c-827b-70c42c6cd7c8",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a58ec61535cd230c92ea770ec5f3e244",
     "grade": false,
     "grade_id": "cell-4ed184ef97131b35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a RegressionEvaluator that uses the [Root Mean Squared Error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) as our performance metric. Import the right package first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf11d6db-f800-423d-a9d3-b4069a499762",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9a3f3646-08ba-4e42-95a0-071a0a1947c5",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9753990f3a78afe1f0ce7170ee768f6c",
     "grade": false,
     "grade_id": "cell-a292d5c29d2d317c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a variable `evaluator` with a rmse as metric for the prediction column given by `gbt`.\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Appliances\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "68c736fc-6fef-46f5-a18f-bdc11cc91363",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ef34bfd6d4b77c178d862bdd6c67136",
     "grade": false,
     "grade_id": "cell-85da592a82153cc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can now create a CrossValidator `cv` that uses the `gbt` as estimator, as well as the evaluator and grid we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eabdb579-27d7-4095-aa2f-becbc570d334",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "546ad288c8f0f53d93e9805f36a9a3e2",
     "grade": true,
     "grade_id": "cell-ff22f5dbf35b6e2f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid,\n",
    "                          evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Appliances\", predictionCol=\"prediction\"),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a42d940-1733-4ece-b0a0-4f43c7294045",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "749133c857046d1bc2e11ebf65eeada1",
     "grade": false,
     "grade_id": "cell-2b33520df40a8ee9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Step 4. Create a Pipeline (`pipeline`) that contains the two stages: `vectorAssembler` and `cv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7eb6e833-879b-4b94-b4d9-4cfd366def4e",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a93c0e13420d8c7ddf6408fd963ca1e3",
     "grade": true,
     "grade_id": "cell-c772c77b96ea0299",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, cv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b85d3e5d-b9c1-48a7-9561-34b5dbdddfa6",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9c8a01711759f1b834f1778033cb9b3",
     "grade": false,
     "grade_id": "cell-83983f3ad8703821",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Step 5. Finally, `fit` the model, and store in a `pipelineModel` variable. This might take quite a bit of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "494ba5ae-fcf2-4592-9d18-533edb19d229",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82cacba754d5f00e687e19bbad0ec903",
     "grade": true,
     "grade_id": "cell-4865af895712af5e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d80726b9-3ac1-4cec-a1a2-85d0a09cd7b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It could be a good idea to save this to disk, in case it takes too long, so we can read it later. Remember to save it to your group's storage account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e34020c1-3c41-409a-85ea-bcfc3588add0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pipelineModel.save(\"/mnt/pirates-of-the-carribbytes/Model-lab4_sahil_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a79035ff-421b-42e1-b561-5d5e0ef207f7",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fa1fc36e0f0972102cd0635baf8f304",
     "grade": false,
     "grade_id": "cell-77849593b37442d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Evaluate the results [10 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f88be02c-0cf1-4c09-ba40-f6e6246b06a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To obtain the predictions in the test set, apply the method `transform()` of the trained pipeline on the test DataFrame! This will not apply the cross-validation of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89046b2e-523e-4dc3-98e4-39b08479cdc1",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12bd28a5ab908cbbd54f5d83be53c812",
     "grade": false,
     "grade_id": "cell-d2a232d67c0b332b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a variable `predictions`:\n",
    "prediction = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "825b1189-ba43-430e-8f66-ddb06c94be6a",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16cf332bde6d4413e31ab0d18074be4f",
     "grade": false,
     "grade_id": "cell-a868d56e37754041",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is easier to view the results when we limit the columns displayed to:\n",
    "\n",
    "- `Appliances`: the consumption of the Appliances in Wh\n",
    "- `prediction`: our predicted prediction\n",
    "\n",
    "Find a way to show the output only for these two features (only for the first 5 rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "107f0b7c-5aed-4fde-9391-1c9801831768",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9ddd0f690a496176b60d3ffd625c60b",
     "grade": true,
     "grade_id": "cell-9846c3a76797db6e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "prediction.select(\"Appliances\",\"prediction\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e49c390-7d75-4f07-84dc-c6972aab486e",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f79ba06b3b7ef738dbad136fbf046e0c",
     "grade": false,
     "grade_id": "cell-f37cc71440a8edf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Are these results any good? Let's compute the RMSE using the evaluator we created before! Store the result in a `rmse` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cdf3cc94-a25d-4d1d-baad-853369c44c04",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ef11de44ab722e8d128816e771dee1e",
     "grade": true,
     "grade_id": "cell-1250797f2aae0195",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6d8c231-3bc4-4353-a4c1-365f63cd2807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0cdf5d62-b461-4ef5-8ce7-34a0b4e66f60",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f5e0f65ad9d1b9cd59fa7aaa0079c58",
     "grade": false,
     "grade_id": "cell-a6d64a5f7cdccf1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Seems a bit high?  Well, this number is closer to what it is reported in the original paper with RF (RMSE around 69).\n",
    "\n",
    "But maybe you can investigate a bit more if you can improve that. Can you find out the importance of the features from the GBTs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6da92ea-f875-44a6-890c-fb5ffa07b0b8",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68a14ed17d1a59fd2fc9537279a13736",
     "grade": false,
     "grade_id": "cell-98af8ec7cf0709ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You first need to find out the best model!! In the way we trained the pipeline, you can find the trained cvModel as one of the stages of the `pipelineModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6ab58ec-e730-4e2b-84e3-5cf19d506e1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cvModel = pipelineModel.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ebe45f5a-35bb-4add-8d59-8f214d16d4fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ac7df49-4e64-4cec-bb8d-8f45b11135f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Check the feature importances of that `cvModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ade0a009-0f35-4d82-b36f-012d6935d5cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cvModel.bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b289b5b1-a315-4a96-89f8-6daae249f1d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Uhm, looks like our model gave feature #28 quite a bit of importance, which is the hour of the day if I recall correctly.  Features #25 and #26 were random, and the model noticed that 26 was completely useless, but gave some importance to 'rv1'. GBTs perform somehow an implicit feature selection, so those low importance features won't affect much their performance, but I wonder if we could just remove low importance features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "52e2eddd-7e37-41bb-9e06-ce0fb567aff1",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "387f5e9cea0dc12361fe6dff8638445c",
     "grade": false,
     "grade_id": "cell-397368093280ee02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Task*: Create a list of those features with less than for example 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f8794b11-bb92-459e-b613-251304638035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pairs = cvModel.bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "66a5ba59-8618-449b-8966-70387720900d",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "867337febcb76e54ad8d895dc6fe3139",
     "grade": true,
     "grade_id": "cell-6b1e6b9006205fb3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a list `to_remove` that contains the feature names that must be removed because their confidence is less than 0.05 \n",
    "to_remove = []\n",
    "\n",
    "for i in range(len(pairs)):\n",
    "  if pairs[i]<0.05:\n",
    "    to_remove.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb989970-db15-4a8b-b355-07cc91bf680d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d06619c9-6f88-446b-9e67-d6e604c9e68e",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06dcc50590c58211670c7d1c71d8e6aa",
     "grade": false,
     "grade_id": "cell-32a9d061f3e5a666",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Removing low importance features [10 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8ff25103-49e1-47e0-9b26-65cf628cb398",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8982e88b3b7cf537087ca9bf62568aa",
     "grade": false,
     "grade_id": "cell-c6802214fa240a6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check the current schema of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "405c2b4e-14d9-4fda-a58e-b1429872eb73",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21398aafc0621aaaeeb1fefd9bf33808",
     "grade": true,
     "grade_id": "cell-7eaf394a6020cdcd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9f8adc17-5b52-40c9-b5db-f28909962590",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c99759a0ca11680f487af8069bc4e3d",
     "grade": false,
     "grade_id": "cell-8b82deb2dd980d57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You don't need to remove the columns from train and test partitions as you won't be able to re-train the pipeline without modifying the VectorAssemble. You simply need to indicate the columns you want to use when creating a new `vectorAssembler2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "173564f7-ed65-4e96-a101-b843d556f4b4",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2e765a19df205df05c2bfa10a68ceea",
     "grade": true,
     "grade_id": "cell-9452a5dfa044d94e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorAssembler2 = VectorAssembler(inputCols=[\"T2\", \"T3\", \"RH_3\", \"T9\", \"dayofyear\", \"hour\"], outputCol=\"features\")\n",
    "vectorAssembler2.transform(train).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "25f964a9-b48e-493f-b68e-e4a32b8844b9",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a33293e5f47ea37aba1f604d7b2ff33b",
     "grade": false,
     "grade_id": "cell-b545a033a4bed177",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "and create a new pipeline, `pipeline2`, with that new `vectorAssemble2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ad6e43d-0f51-43e8-a767-0147671c4d29",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76db7fbdde7bf0e9a426ffcdb1ec7040",
     "grade": true,
     "grade_id": "cell-d4a3e8633542f9e6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline(stages=[vectorAssembler2, cv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15b6e663-48f1-43d3-9650-e68fe9abf5aa",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f076896e775194a1828f8508640bfa43",
     "grade": false,
     "grade_id": "cell-29d90e5cdfccfd71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "d\n",
    "And `fit` the new pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b0439f3f-8e06-40ce-8f52-d1fb187e6b5c",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23057298ae3e9fd642b3ae25efc2145f",
     "grade": true,
     "grade_id": "cell-45887160f551fcad",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipelineModel2 = pipeline2.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5187502a-15ac-4e5c-ba91-c78a6f917e46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, make predictions and compute the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "73855349-7366-40da-909a-e86b7bc28155",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8701482b4062335061a2eb764002993",
     "grade": true,
     "grade_id": "cell-5c708eda14fa475e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "prediction2 = pipelineModel2.transform(test)\n",
    "rmse2 = evaluator.evaluate(prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "02fe9d5d-9197-4e7c-96ab-f09877394735",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92f92325-b14b-4904-a638-ea0a6c260727",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "131d2bf7444af671154be9c460455b0b",
     "grade": false,
     "grade_id": "cell-fd71263ad7f3c709",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check the features of the best model and feature importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0c284332-7e7f-4118-9acc-246d285bcea4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cvModel2 = pipelineModel2.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69229c84-4d4b-451c-8f75-3a2f03c6e7e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cvModel2.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ae062ece-397d-49f4-a9a6-49d25e2f4606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cvModel2.bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d2206671-8920-44d6-b939-f8ab5548b9a5",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2edc686f1ee60293d728331abe08958d",
     "grade": false,
     "grade_id": "cell-d0777fc1ab7a64af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In my case, I got the same performance as before, but this time my model only considered 3 input features! This might not have made our model more precise (Because GBTs already ignored those features), but makes it more interpretable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cfb8f1f8-14d8-46ef-a69e-34233152792d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Improving further your model [20 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "061aa913-7b83-4e8b-986b-c4d222fdf8a4",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95987f6845ee31a2a836ffb6848a390b",
     "grade": false,
     "grade_id": "cell-ffeb040b4832b189",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There might be many ways to improve the results we obtained here. \n",
    "\n",
    "A few ideas for you to think about:\n",
    "\n",
    "- Parameter tuning: I have used a relatively small set of parameters, and I haven't investigated what happened in training and test, is there overfitting of the training? would we be able to use a large number of trees?\n",
    "- The features of this dataset are numerical, are there other classifiers that may be more appropriate than GBTs?\n",
    "- We haven't really done any careful pre-processing of the data. Are there outliers or noise that might be having an impact in the results? \n",
    "- Do we need any normalisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe0d5ad2-a774-48a1-ae35-88c2a01a4807",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Check for Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4f2385a-f0a1-41eb-824c-f7711092fd42",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c2c3bbf137d07c19ab4af009c981563",
     "grade": true,
     "grade_id": "cell-4fe4f316d22ce863",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "prediction_train = pipelineModel2.transform(train)\n",
    "rmse_train = evaluator.evaluate(prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "400d5ee3-0b30-4d62-a31a-3fd7d8d04fd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rmse2)\n",
    "print(rmse_train) # so not overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5fe0cb61-4856-4c5f-8f19-00a1ad4bbd58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Normalise and Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ce1afa9-3568-45bd-a977-6e1b9ed52de4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find outliers - box plot\n",
    "boxplot = df.toPandas().boxplot(column=[\"T2\", \"T3\", \"RH_3\", \"T9\", \"dayofyear\", \"hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0c4bed8-5d90-4846-9e80-5f8e240d3cd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove outliers functions\n",
    "def remove_outliers(df_in, cols):\n",
    "  for col in cols:\n",
    "    quantiles = df_in.approxQuantile(col, [0.25, 0.75], 0)\n",
    "    if ((quantiles) and (quantiles[1]!=quantiles[0])):\n",
    "      iqr = quantiles[1]-quantiles[0] #Interquartile range\n",
    "      fence_low  = quantiles[0]-1.5*iqr\n",
    "      fence_high = quantiles[1]+1.5*iqr\n",
    "      df_in = df_in.filter((df_in[col]>fence_low) & (df_in[col]<fence_high))\n",
    "  return df_in\n",
    "\n",
    "def remove_outliers_iteratively(df, cols):\n",
    "  while (True):\n",
    "    len1 = df.count()\n",
    "    df = remove_outliers(df, cols)\n",
    "    len2 = df.count()\n",
    "    if (len1==len2):\n",
    "      break\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec47b547-3323-4192-864d-67e6d0700f4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Normalizer function\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def normalizeDF(df, cols):\n",
    "  # UDF for converting column type from vector to double type\n",
    "  unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "\n",
    "  # Iterating over columns to be scaled\n",
    "  for i in cols: #[ \"T2\", \"T3\", \"RH_3\", \"T9\", \"dayofyear\", \"hour\"]:\n",
    "      # VectorAssembler Transformation - Converting column to vector type\n",
    "      assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "      # MinMaxScaler Transformation\n",
    "      scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "      # Pipeline of VectorAssembler and MinMaxScaler\n",
    "      pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "      # Fitting pipeline on dataframe\n",
    "      df = pipeline.fit(df).transform(df)\\\n",
    "                                .withColumn(i+\"_Scaled\",unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "64ef49cb-c865-462e-85e9-19f0666310da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# normalise then remove outliers in train\n",
    "old_df = df\n",
    "df = normalizeDF(df, ['lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4',\n",
    "                      'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9',\n",
    "                      'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility',\n",
    "                      'Tdewpoint', 'rv1', 'rv2', 'dayofyear', 'hour', 'month'])\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b223e564-d8d2-41b8-becc-f81d85dc8861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "train.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61ddb3ea-2d0b-495e-b7ef-7ddcfe3a2ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for col in [\"T2_Scaled\", \"T3_Scaled\", \"RH_3_Scaled\", \"T9_Scaled\", \"dayofyear_Scaled\", \"hour_Scaled\"]:\n",
    "  gre_histogram = train.select(col).rdd.flatMap(lambda x: x).histogram(11)\n",
    "\n",
    "  # Loading the Computed Histogram into a Pandas Dataframe for plotting\n",
    "  pd.DataFrame(\n",
    "      list(zip(*gre_histogram)), \n",
    "      columns=['bin', 'frequency']\n",
    "  ).set_index(\n",
    "      'bin'\n",
    "  ).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d9a3090-c882-4503-8625-eccbe2f51e8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#remove outliers in train BETTER IF NOT DONE\n",
    "'''train = train.select('Appliances','lights_Scaled', 'T1_Scaled', 'RH_1_Scaled', 'T2_Scaled', 'RH_2_Scaled', 'T3_Scaled', 'RH_3_Scaled', 'T4_Scaled',\n",
    "                      'RH_4_Scaled', 'T5_Scaled', 'RH_5_Scaled', 'T6_Scaled', 'RH_6_Scaled', 'T7_Scaled', 'RH_7_Scaled', 'T8_Scaled', 'RH_8_Scaled',                       'T9_Scaled',\n",
    "                      'RH_9_Scaled', 'T_out_Scaled', 'Press_mm_hg_Scaled', 'RH_out_Scaled', 'Windspeed_Scaled', 'Visibility_Scaled',\n",
    "                      'Tdewpoint_Scaled', 'rv1_Scaled', 'rv2_Scaled', 'dayofyear_Scaled', 'hour_Scaled', 'month_Scaled')'''\n",
    "\n",
    "train = train.select(\"Appliances\",\"T2_Scaled\", \"T3_Scaled\", \"RH_3_Scaled\", 'RH_7_Scaled', \"T9_Scaled\",'Windspeed_Scaled', \"dayofyear_Scaled\", \"hour_Scaled\")\n",
    "train.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb6e1d0f-5cc3-4745-983b-052f1add80f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Change Parameters and Build New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15b54688-4c3c-485d-b9dd-33ecf0e0b350",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(labelCol=\"Appliances\", stepSize=0.15, subsamplingRate=1.0, impurity=\"variance\", featureSubsetStrategy=\"all\", validationTol=0.01)\n",
    "\n",
    "# Parmeter experimentation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [2,20,2]) \\\n",
    "    .addGrid(gbt.maxIter, [20,30]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid,\n",
    "                          evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Appliances\", predictionCol=\"prediction\"),\n",
    "                          numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3ebaeff-2e2e-4d3c-8654-803ee54f679f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = train.columns\n",
    "feature_cols.remove(\"Appliances\")\n",
    "vectorAssembler3 = VectorAssembler(inputCols=feature_cols,outputCol=\"features\")\n",
    "pipeline3 = Pipeline(stages=[vectorAssembler3, cv])\n",
    "pipelineModel3 = pipeline3.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cc82ca4e-c5f5-4b7c-8d4c-ce28f1af5bd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction_train2 = pipelineModel3.transform(train)\n",
    "rmse_train2 = evaluator.evaluate(prediction_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "93abad5e-6390-4cdd-90c3-34124f0074d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rmse)\n",
    "print(rmse2)\n",
    "print(rmse_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e96079a5-5d73-4a9c-8144-8f55ba8a3b6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction3 = pipelineModel3.transform(test)\n",
    "rmse3 = evaluator.evaluate(prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "25905340-cb9e-4fbf-b132-00a10b054c7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rmse)\n",
    "print(rmse_train)\n",
    "print(rmse2)\n",
    "print(rmse_train2)\n",
    "print(rmse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e9adc004-9ee5-4341-bb5a-9b86a8b9365b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cvModel3 = pipelineModel3.stages[1]\n",
    "print(cvModel3.bestModel)\n",
    "pairs = cvModel3.bestModel.featureImportances\n",
    "to_remove = []\n",
    "\n",
    "for i in range(len(pairs)):\n",
    "  if pairs[i]<0.05:\n",
    "    to_remove.append(i)\n",
    "print(to_remove)\n",
    "#train.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15e7ce6a-171a-4002-8b30-54306ea540e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e59f19bf-de0c-4c4b-bbfd-07a167c54b65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "20ae7c07-200e-4991-aecb-75ee44413793",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## New Model - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf4d02c5-537c-469f-aafc-9b0468824390",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = old_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "990d5106-3de7-4a47-9521-7e4371848a95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "feature_cols = train.columns\n",
    "feature_cols.remove(\"Appliances\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=feature_cols, outputCol=\"unscaled_features\")\n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "lr = LinearRegression(labelCol=\"Appliances\",)\n",
    "\n",
    "# Parmeter experimentation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.aggregationDepth, [2,3]) \\\n",
    "    .addGrid(lr.maxIter, [100,150,200]) \\\n",
    "    .addGrid(lr.regParam, [0.0,0.05,0.1,0.15]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Appliances\", predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid,\n",
    "                          evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Appliances\", predictionCol=\"prediction\"),\n",
    "                          numFolds=10)\n",
    "\n",
    "stages = [vectorAssembler, standardScaler, cv]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ef020fbc-270c-456b-a9fa-14255fe407af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipelineModel_lr = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "951fd293-9d20-492b-88ff-d8ab03cc368d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction_train_lr = pipelineModel_lr.transform(train)\n",
    "rmse_train_lr = evaluator.evaluate(prediction_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d161636-df25-44ea-a597-5744b9ca3df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction_lr = pipelineModel_lr.transform(test)\n",
    "rmse_lr = evaluator.evaluate(prediction_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "950503dd-ab0d-4988-8f0b-744c6c3c6a0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rmse)\n",
    "print(rmse_train)\n",
    "print(rmse2)\n",
    "print(rmse_train_lr)\n",
    "print(rmse_lr)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "MLLib Lab_sahil",
   "notebookOrigID": 1620590807093829,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "name": "MLPipeline Bike Dataset",
  "notebookId": 3638908530782568
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
